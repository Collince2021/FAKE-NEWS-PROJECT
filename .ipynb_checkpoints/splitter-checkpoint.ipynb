{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab1f3d5-0497-426b-8392-c2812d3a7e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Masini\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully split into 4 files!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"True Clean.csv\")\n",
    "\n",
    "# Split the dataset into 4 equal parts\n",
    "num_splits = 4\n",
    "split_dfs = np.array_split(df, num_splits)\n",
    "\n",
    "# Save each part as a new CSV file\n",
    "for i, split_df in enumerate(split_dfs):\n",
    "    split_df.to_csv(f\"True_Clean_Part{i+1}.csv\", index=False)\n",
    "\n",
    "print(\"Dataset successfully split into 4 files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4200ace-4dd8-4436-801b-2ac1dc692427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Masini\\anaconda3\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing True_Clean_Part1.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                      | 106/5355 [1:14:26<163:50:11, 112.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: bad allocation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                       | 107/5355 [1:14:32<117:34:41, 80.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: bad allocation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                        | 125/5355 [1:29:00<66:08:20, 45.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: bad allocation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                        | 126/5355 [1:29:09<50:11:11, 34.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 943200 bytes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                        | 127/5355 [1:30:02<58:01:32, 39.96s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load MarianMT model for English-to-Swahili translation\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-sw\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/content/sample_data/True Clean.csv\")\n",
    "\n",
    "# Split dataset into two parts\n",
    "df_part1, df_part2 = np.array_split(df, 2)\n",
    "\n",
    "# Function to translate text\n",
    "def translate_text(text):\n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        return text  # Skip empty values\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        outputs = model.generate(**inputs)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating: {e}\")\n",
    "        return text  # Return original text if translation fails\n",
    "\n",
    "# Apply translation to each part\n",
    "text_column = \"text\"  # Ensure this matches your dataset's column name\n",
    "tqdm.pandas()\n",
    "df_part1[\"translated_text\"] = df_part1[text_column].progress_apply(translate_text)\n",
    "df_part2[\"translated_text\"] = df_part2[text_column].progress_apply(translate_text)\n",
    "\n",
    "# Save each translated part\n",
    "df_part1.to_csv(\"/content/sample_data/True_Clean_Part1_Translated_Swahili.csv\", index=False)\n",
    "df_part2.to_csv(\"/content/sample_data/True_Clean_Part2_Translated_Swahili.csv\", index=False)\n",
    "\n",
    "print(\"Dataset successfully split, translated, and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6dcc8-1b5b-48d0-baa0-468713728ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
